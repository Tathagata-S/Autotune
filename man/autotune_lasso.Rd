% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autotune_lasso.R
\name{autotune_lasso}
\alias{autotune_lasso}
\title{Autotune LASSO: Fitting a linear model with fast and automatic lasso regularization}
\arguments{
\item{xin}{Matrix of predictors, with one column for each predictor.
Dimension will be \code{nobs} \eqn{\times} \code{nvars}; so each row is a new observation
and \code{nvars} > 1.}

\item{yin}{Vector of responses.}

\item{alpha}{Default 0.01, significance level of sequential F-tests 
used for estimation of support set.}

\item{standardize}{Logical flag for standardization of all variables in x, prior to
fitting the model sequence. The coefficients are always returned on the
original scale. Default value is \code{TRUE}.}

\item{standardize_response}{Logical flag for demeaning the reponse variable y.
Default value is \code{TRUE}.}

\item{intercept}{Should intercept(s) be fitted (default=\code{TRUE}) or set to zero
(\code{FALSE}).}

\item{active}{Should active set selection be used (\code{TRUE}) or avoided
(default = \code{FALSE}). It is under experimentation phase, use it 
with care.}

\item{trace_it}{Logical input, default \code{FALSE}; if \code{TRUE}
prints out the iteration number while running, useful for big datasets that 
take a long time to fit.}

\item{tolerance}{Numeric input for an additional stopping criteria on the
coordinate descent when sigma is updating. Stops that coordinate
descent when the relative change between successive iterates of
coefficients' estimates is less than the \code{tolerance}. Default value 
is \eqn{10^{-4}}.}

\item{beta_tolerance}{Numeric input for the stopping criteria on the
coordinate descent when sigma is not updating. Stops that coordinate
descent when the relative change between successive iterates of
coefficients' estimates is less than the \code{beta_tolerance}.
Default value is \eqn{10^{-3}}.}

\item{iter_max}{Maximum number of iterations of coordinate descent
allowed when sigma is updating. Default value is 30.}

\item{beta_iter_max}{Maximum number of iterations of coordinate 
descent allowed when sigma is not updating. Default value is 40.}

\item{PR_norm_l2}{Logical flag to whether use the l2 norm of partial 
residuals for ordering them instead of the default l1 norm.}
}
\value{
A list with various intermediate and final outputs produced by Autotune LASSO in its regularization path.

\item{beta}{ Final estimates of regression coefficients. }
\item{a0}{ Intercept of the fit. }
\item{lambda}{Final thresholding value \eqn{\lambda =
\lambda_0\hat\sigma^2} used in the coordinate descent after noise
variance estimate \eqn{\hat\sigma^2} has converged.}
\item{sigma_sq}{Final estimate of noise variance \eqn{\sigma^2}}
\item{CD.path.details}{ A list of additionals details about the coordinate descent path taken by
Autotune lasso:}
\itemize{
   \item{\code{sorted_predictors:}}{       Decreasing ordering of predictors in terms of
their contribution to predicting the response values.}
\item{sigma_sq_seq:}{ A \code{no_of_iterations} length sequence of estimates
noise variance \eqn{\sigma^2}. }
    \item{\code{no_of_iter_before_lambda_conv:}}{      Number of iterations of coordinate descent performe
before noise variance estimate \eqn{\hat{\sigma}^2} converged.}
\item{\code{no_of_iter_after_lambda_conv:}}{     After noise variance estimate \eqn{\hat{\sigma}^2} has
converged, it is the number of iterations of coordinate descent required
for coefficients \eqn{\hat\beta} to converge.}
\item{\code{no_of_iterations:}}{     Total of iterations of coordinate descent implemented by Autotune Lasso}
    \item{\code{lambda0:}}{        Value of \eqn{\lambda_0} used in the Autotune LASSO.
Refer to the original paper for details.} 
   \item{\code{support_set:}}{       Final set of predictors included in the support set for
    sigma estimation by autotune lasso.}
\item{\code{count_sig_beta:}}{ (\code{no_of_iterations})-length vector containing the
support set sizes across the coordinate descent iterations while the
noise variance estimate is being updated.}
\item{\code{null_support:}{ Boolean output indicating whether final estimate of 
support set came out to be a null set. In case it happens, \code{autotune_lasso} uses the
estimate of support set in the previous iteration for getting the 
final estimate of noise variance \eqn{\sigma^2}}}
  }
}
\description{
Fits a linear model via alternative optimization of penalized gaussian maximum likelihood 
which is a biconvex function of regression coefficients \eqn{\beta} and noise variance \eqn{\sigma^2}. 
Autotune LASSO's regularization path quickly picks out a good lambda for LASSO and then
returns the corresponding linear fit along with various attributes related to the fit.
}
\examples{
set.seed(10)
n <- 80
p <- 400
s <- 5
snr <- 4
betatrue <- c(rep(1,s), rep(0, p - s))
x <- matrix(rnorm(n * p), ncol = p)
error.sd <- sqrt((betatrue \%*\% betatrue)/snr)
err <- rnorm(n, sd = error.sd)
y <- x \%*\% betatrue + err
ans <- autotune_lasso(x, y, trace_it = TRUE)
b <- betatrue
# The Predictors which are actually significant:
which(b != 0)
# The Predictors which had nonzero estmated coefficients:
which(ans$beta != 0)
# Top 10 predictors X_i's in the ranking of X_i's given by autotune:
ans$CD.path.details$sorted_predictors[1:10]
# No of significant predictors in each CD iteration when sigma_hat is allowed to vary:
ans$CD.path.details$count_sig_beta
# Sigma estimates in each CD iteration:
ans$CD.path.details$sigma_sq_seq
# Empirical noise variance:
var(err)


}
