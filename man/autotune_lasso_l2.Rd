% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{autotune_lasso_l2}
\alias{autotune_lasso_l2}
\alias{Autotune_l2}
\title{Autotune LASSO with \eqn{\ell_2} norm of partial residuals: Fitting
 a linear model with fast and automatic lasso regularization}
\usage{
autotune_lasso_l2(
  x,
  y,
  alpha = 0.01,
  lambda = NULL,
  tolerance = 1e-04,
  beta_tolerance = 0.001,
  iter_max = 30L,
  beta_iter_max = 40L,
  verbose = FALSE
)
}
\arguments{
\item{x}{Matrix of predictors, with one column for each predictor.
Dimension will be \code{nobs} \eqn{\times} \code{nvars}; so each row is a new observation.}

\item{y}{Vector of responses.}

\item{alpha}{Default 0.01, significance level of sequential F-tests
used for estimation of support set.}

\item{lambda}{Numeric input, default (and recommended) value None.
If user provides a \eqn{\lambda}, autotune lasso starts finding pathwise solutions
from that \eqn{\lambda}, else sets the starting \eqn{\lambda = \|x^T y\|_{\infty}/(nobs \times Var(y))}}

\item{tolerance}{Numeric input for an additional stopping criteria on the
coordinate descent when sigma is updating. Stops that coordinate
descent when the relative change between successive iterates of
coefficients' estimates is less than the \code{tolerance}.}

\item{beta_tolerance}{Numeric input for the stopping criteria on the
coordinate descent when sigma is not updating. Stops that coordinate
descent when the relative change between successive iterates of
coefficients' estimates is less than the \code{beta_tolerance}.}

\item{iter_max}{Maximum number of iterations of coordinate descent
allowed when sigma is updating}

\item{beta_iter_max}{Maximum number of iterations of coordinate descent
allowed when sigma is not updating}

\item{verbose}{Logical input, default \code{FALSE}; if \code{TRUE}
prints out the following details of Autotune LASSO path:
number of iterations of coordinate descent when sigma was updating,
number of iterations of coordinate descent when sigma was not updating,
and the size of the estimated support set(or the number of outliers
identified in the final distribution of \eqn{\ell_1} norm of partial
residuals)}
}
\value{
A list with various intermediate and final outputs
 produced by Autotune LASSO in its regularization path.
 \item{resi_mat}{A (\code{no_of_iter} + 1)\eqn{ \times nobs} matrix
 consisting of residuals after the end of each iteration of
 coordinate descent while sigma was updating and the last row
 have the final residuals after coordinate descent has converged.}
 \item{beta_mat}{A (\code{no_of_iter} + 1)\eqn{ \times nvars} matrix
 consisting of coefficients after the end of each iteration of
 coordinate descent while sigma was updating and the last column
 have the final estimates of coefficients after coordinate descent has converged.}
\item{sigma2_seq}{ A \code{no_of_iter} length sequence of estimates
noise variance \eqn{\sigma^2}.}
\item{beta}{Final estimates of regression coefficients.}
\item{lambda}{Value of \eqn{\lambda_0} used in the Autotune LASSO.
Refer to the original paper for details.}
\item{lambda_for_soft_thres}{Final thresholding value \eqn{\lambda =
\lambda_0\hat{\sigma}^2} used in the coordinate descent after noise
variance estimate \eqn{\hat{\sigma}^2} has converged.}
\item{sorted_predictors}{Decreasing ordering of predictors in terms of
their contribution to predicting the response values.}
\item{support_set}{Final set of predictors included in the support set
by autotune lasso.}
\item{no_of_iter}{Number of iterations of coordinate descent performed
before noise variance estimate \hat{\sigma}^2 converged.}
\item{no_of_beta_iter}{After noise variance estimate \hat{\sigma}^2 has
converged, it is the number of iterations of coordinate descent required
for coefficients \eqn{\hat\beta} to converge.}
\item{count_sig_beta}{(\code{no_of_iter})-length vector containing the
support set sizes across the coordinate descent iterations while the
noise variance estimate is being updated.}
\item{null_support}{Logical value whether an empty support set was encountered
by autotune LASSO. In such a case, the noise variance is fixed at its
estimate in the previous iteration of coordinate (when support set was not
empty) and the coordinate descent is run till the coefficents converge
\eqn{\hat\beta}. }
}
\description{
Fits a linear model via penalized maximum likelihood-inspired approach. Autotune
LASSO's regularization path quickly picks out a good lambda for LASSO and then
returns the corresponding linear fit along with various attributes related to the
fit. Difference between this function and \code{autotune_lasso} is that
it uses the \eqn{\ell_2} norm of partial residuals instead of \eqn{
\ell_1} norm for sorting the predictors in inner loop. Details given in the
paper.
}
\examples{
set.seed(10)
n <- 80
p <- 500
s <- 5
type <- 1
snr <- 3
betatrue <- c(rep(1,s), rep(0, p - s))
x <- scale(matrix(rnorm(n * p), ncol = p))
error.sd <- sqrt((betatrue \%*\% betatrue)/snr)

err <- rnorm(n, sd = error.sd)
y <- x \%*\% betatrue + err
y <- y - mean(y)
ans <- autotune_lasso_l2(x, y, verbose = TRUE)

b <- betatrue
cat("The Predictors which are actually significant:", which(b != 0), "\n")
cat("Top 10 predictors X_i's in the ranking of X_i's", "\n", " given by autotune:", (ans$sorted_predictors[1:10] + 1))
cat("No of significant predictors in each CD iteration", "\n", " when sigma_hat is allowed to vary:", ans$count_sig_beta)
cat("Sigma estimates in each CD iteration:", ans$sigma2_seq)
cat("Empirical noise variance:", var(err))
which(ans$beta != 0)
which(b != 0)
}
